%%%%%%%%%%%%%%%%%%%%%%% file template.tex %%%%%%%%%%%%%%%%%%%%%%%%%
%
% This is a general template file for the LaTeX package SVJour3
% for Springer journals.          Springer Heidelberg 2010/09/16
%
% Copy it to a new file with a new name and use it as the basis
% for your article. Delete % signs as needed.
%
% This template includes a few options for different layouts and
% content for various journals. Please consult a previous issue of
% your journal as needed.
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% First comes an example EPS file -- just ignore it and
% proceed on the \documentclass line
% your LaTeX will extract the file if required
%
\RequirePackage{fix-cm}
\RequirePackage{amsmath}
\documentclass{svjour3}                     % onecolumn (standard format)
%\documentclass[smallcondensed]{svjour3}     % onecolumn (ditto)
%\documentclass[smallextended]{svjour3}       % onecolumn (second format)
%\documentclass[twocolumn]{svjour3}          % twocolumn
%
\smartqed  % flush right qed marks, e.g. at end of proof

\usepackage{graphicx}
%\usepackage{amsmath}
\usepackage{mathptmx}
%\usepackage{newtxtext,newtxmath}
\let\proof\relax
\let\endproof\relax
%\usepackage{amsthm}
%\usepackage{amssymb}
\usepackage{stmaryrd}
\usepackage{natbib}
\usepackage{enumitem}
\usepackage{times}
\usepackage{graphicx}
\usepackage{latexsym}
\usepackage{bussproofs}
\usepackage{pgf}
\usepackage{adjustbox}
\usepackage{xcolor}



\DeclareMathSymbol{\Gamma}{\mathalpha}{operators}{0}
\DeclareMathSymbol{\Delta}{\mathalpha}{operators}{1}
\DeclareMathSymbol{\Theta}{\mathalpha}{operators}{2}
\DeclareMathSymbol{\Lambda}{\mathalpha}{operators}{3}
\DeclareMathSymbol{\Xi}{\mathalpha}{operators}{4}
\DeclareMathSymbol{\Pi}{\mathalpha}{operators}{5}
\DeclareMathSymbol{\Sigma}{\mathalpha}{operators}{6}
\DeclareMathSymbol{\Upsilon}{\mathalpha}{operators}{7}
\DeclareMathSymbol{\Phi}{\mathalpha}{operators}{8}
\DeclareMathSymbol{\Psi}{\mathalpha}{operators}{9}
\DeclareMathSymbol{\Omega}{\mathalpha}{operators}{10}


\DeclareFontFamily{U} {MnSymbolA}{}

\DeclareFontShape{U}{MnSymbolA}{m}{n}{
  <-6> MnSymbolA5
  <6-7> MnSymbolA6
  <7-8> MnSymbolA7
  <8-9> MnSymbolA8
  <9-10> MnSymbolA9
  <10-12> MnSymbolA10
  <12-> MnSymbolA12}{}
\DeclareFontShape{U}{MnSymbolA}{b}{n}{
  <-6> MnSymbolA-Bold5
  <6-7> MnSymbolA-Bold6
  <7-8> MnSymbolA-Bold7
  <8-9> MnSymbolA-Bold8
  <9-10> MnSymbolA-Bold9
  <10-12> MnSymbolA-Bold10
  <12-> MnSymbolA-Bold12}{}

\DeclareSymbolFont{MnSyA}{U}{MnSymbolA}{m}{n}

\DeclareMathSymbol{\twoheaduparrow}{\mathop}{MnSyA}{25}




\makeatletter

% % % % % % % % % % % % % % % Internal Commands % % % % % % % % % % % % %
\newcommand{\raisemath}[1]{\mathpalette{\raisem@th{#1}}}
\newcommand{\raisem@th}[3]{\raisebox{#1}{$#2#3$}}

\newcommand{\bigperpp}{%
  \mathop{\mathpalette\bigp@rpp\relax}%
  \displaylimits
}
\newcommand{\bigp@rpp}[2]{%
  \vcenter{
    \m@th\hbox{\scalebox{\ifx#1\displaystyle1.3\else1.3\fi}{$#1\perp$}}
  }%
}
\newcommand{\bigperp}{\raisemath{.5pt}{\bigperpp}}

\newcommand{\nm}{\,\mid\!\sim\,}
\newcommand{\ssim}{% 
     \setbox0=\hbox{$\sim$}%
     \adjustbox{width=8pt,height=\height}{$\sim$}
}
\newcommand{\Uuparrow}{% 
     \setbox0=\hbox{$\scriptstyle\Uparrow$}%
     \raisebox{.2ex}{$\scriptstyle\Uparrow$}
}
\newcommand{\uuparrow}{% 
     \setbox0=\hbox{$\scriptstyle\uparrow$}%
     \raisebox{.2ex}{$\scriptstyle\uparrow$}
}
\newcommand{\thuarrow}{% 
     \setbox0=\hbox{$\scriptstyle\twoheaduparrow$}%
     \raisebox{.2ex}{$\scriptstyle\twoheaduparrow$}
}
% % % % % % % % % % % % some superscript commands (dont use often) % % % % % % % %
%\newcommand{\uw}[1]{{#1}^{\!\scriptscriptstyle\uparrow W}}
%\newcommand{\uww}[1]{{#1}^{\!\scriptscriptstyle\uparrow W'}}
%\newcommand{\uuw}[1]{{#1}^{\!\scriptscriptstyle\Uparrow W}}
%\newcommand{\uuww}[1]{{#1}^{\!\scriptscriptstyle\Uparrow W'}}
\newcommand{\dhu}[1]{{#1}^{\!\!\!\scriptstyle\twoheaduparrow}}

% % % % % COMMANDS FOR NON-MONOTONIC CONSEQUENCES % % % % % % % % %l
\newcommand{\nmc}{\mathbin{\mid\joinrel\!\!\ssim}}
\newcommand{\nme}{\mathrel{\nmc_{\!\!\!\!e\,}}}

\newcommand{\qmc}[4][\Gamma,]{{#1#2\,\,\mathrel{\nmc}}^{\!\!\!\!\!\!\!\uuparrow\scriptstyle #4}\,\,#3}
\newcommand{\nqmc}[4][\Gamma,]{{#1#2\mathrel{\not\nmc}}^{\!\!\!\!\!\!\!\uuparrow\scriptstyle #4}\,\,#3}
\newcommand{\src}[4][\Gamma,]{{#1#2\,\,\mathrel{\nmc}}^{\!\!\!\!\!\!\!\Uuparrow\scriptstyle #4}\,\,#3}
\newcommand{\gsrc}[3][\Gamma,]{{#1#2\,\,\mathrel{\nmc}}^{\!\!\!\!\!\!\thuarrow}\,\,#3}
\newcommand{\gsrce}[3][\Gamma,]{#1#2\,\,\mathrel{\nme}^{\!\!\!\!\!\!\!\!\thuarrow\,\,}\,#3}
\newcommand{\ngsrc}[3][\Gamma,]{{#1#2\,\,\mathrel{\not\nmc}}^{\!\!\!\!\!\thuarrow\scriptstyle}\,\,#3}


% % % % % % % % % % % % % % % % % Incomaptibility Set % % % % % % % % % % % % %
\newcommand{\Bigtheta}{% 
     \setbox0=\hbox{$\Theta$}%
     \scalebox{1.4}{$\Theta$}
}

\newcommand{\sris}[2]{\raisemath{-.5ex}{\Bigtheta}^{\!\!\raisemath{2pt}{\scriptscriptstyle #1}}_{\!\!\raisemath{-2pt}{\,\scriptscriptstyle #2\,}}} 

% % % % % % Degree Command % % % % % % % % % % % % % % % % % % % %
\newcommand{\degree}{\ensuremath{^\circ}}


\renewcommand\vec{\mathaccent"017E}

\makeatother

 

\journalname{Synthese}
%\renewcommand{\@cite}[1]{#1}
%\usepackage[natbibapa]{apacite}
\usepackage[hang,flushmargin]{footmisc} 
\usepackage[hidelinks]{hyperref}
%\usepackage{lingmacros}
\hypersetup{
    colorlinks=false,
    pdfborder={0 0 0},
}
\newcommand\pref[1]{(\ref{#1})}




\begin{document}

\title{The New Explanatory Inferentialism%\thanks{Grants or other notes
%about the article that should go on the front page should be
%placed here. General acknowledgments should be placed at the end of the article.}
}
%\subtitle{Do you have a subtitle?\\ If so, write it here}

%\titlerunning{Short form of title}        % if too long for running head

\author{Kareem Khalifa        \and
        Jared Millson				\and
        Mark Risjord
}

%\authorrunning{Short form of author list} % if too long for running head

\institute{Kareem Khalida \at
              first address \\
              Tel.: +123-45-678910\\
              Fax: +123-45-678910\\
              \email{fauthor@example.com}           %  \\
%             \emph{Present address:} of F. Author  %  if needed
           \and
           Jared Millson \at
              second address
           \and 
           Mark Risjord \at
              third address
}

\date{Received: date / Accepted: date}
% The correct dates will be entered by the editor
\raggedbottom

\maketitle

\begin{abstract}
Abstract
\end{abstract}



\section{Re-Carving the Explanation Literature}
\label{sec:background}
Let \textit{explanatory inferentialism} (EI) denote the position that the explanatory relation is fundamentally an inferential relation. Below, we introduce a separate, \textit{semantic inferentialism}, in which the meaning of an expression is its inferential role. While Hempel (1965) is the paradigmatic explanatory inferentialist, the unificationists whom he strongly influenced also deserve this mantle (Friedman 1974; Kitcher 1989; Schurz 1999; Schurz and Lambert 1994). So far as we know, none of these explanatory inferentialists also endorse semantic inferentialism. At the very least, none of them articulate a very clear relationship between explanatory and semantic inferentialism, as we shall do below.

\subsection{Our Predecessors}
\label{subsec:predecessors}
Let's briefly rehearse some details of these earlier explanatory inferentialists. Hempel's covering law model---consisting of deductive-nomological (DN), inductive-statistical (IS), and deductive-statistical (DS) explanations---stands as the ``fountainhead'' of contemporary theories of explanation. A DN explanation of $E$ is a sound argument of the form $C_1, \dots, C_k, L_1, \dots, L_r  \vdash E$, where $L_1, \dots, L_r$ are universal laws of nature and are indispensable to the argument's soundness. IS-explanations are the same, save that $L_1, \dots, L_r$ are statistical laws, and, when paired with $C_1, \dots, C_k$, the resulting argument must be inductively strong rather than deductively valid. DS-explanations are sound deductive arguments, but have as their explananda ($E$) a statistical law, and contain at least one statistical law among $L_1, \dots, L_r$. 

While the aforementioned unificationists differ on technical details, we will briefly discuss Kitcher's view, as an illustration. Unlike Hempel, Kitcher takes explanations to be more than mere arguments (i.e. premises and a conclusion). For Kitcher, explanations are derivations: sequences of statements that proceed from a set of premises to a desired conclusion according to a set of inference rules. These derivations must be part of an ``explanatory store'' that best systematizes the total corpus of accepted statements in a scientific community. 

To get a sense in what this systematization consists, some of Kitcher's technical notions are needed. Each explanatory derivation is an instance of a more abstract \textit{schematic argument}, which is a sequence of unsaturated or \textit{schematic sentences}. Each schematic sentence has a set of \textit{filling instructions}, specifying the domains over which the different variables in the schematic sentences quantify. The schematic argument becomes a general \textit{argument pattern} when it is paired with filling instructions and a \textit{classification}, the latter of which specifies the premises of the schematic argument, the inference rules used to proceed from one sentence in the sequence to the next, etc.
A systemization of the corpus of accepted statements $K$ is any set of general argument patterns that derives some members of $K$ from others. Explanation consists of using instances of the ``best'' systemization, $E(K)$, as measured according to the following criteria:
\begin{enumerate}
	\item \textit{Acceptability}: Each step of each instance of a general argument pattern in $E(K)$ must be deductively valid, and acceptable relative to $K$.
	\item \textit{Scope}: Unification increases in proportion to the size of the conclusion set of the number of acceptable instances of $E(K)$.
	\item \textit{Stringency}: Unification increases in proportion to the strictness of the filling instructions and classifications in $E(K)$.
	\item \textit{Number of patterns}: Unification decreases in proportion to the number of general argument patterns in $E(K)$.
\end{enumerate}
Kitcher does not develop an account of what to do in the case of tradeoffs. This will not matter in what follows.


\subsection{The Alternatives}
\label{subsec:alternatives}
Explanatory \textit{non-inferentialists} deny that the explanatory relation is fundamentally inferential in nature. Typically, non-inferentialists hold that explanatory relationships represent some objective dependency relationship between events, facts, objects, or states of affairs. Note that it is possible for non-inferentialists to grant that the explanatory relationship is necessarily inferential in character, but deny that it is \textit{fundamentally} so, e.g. because they hold that the inferential structure is an indispensable means to the ultimate explanatory goal of representing dependency relationships\footnote{Strevens (2008) might be interpreted as advancing a view of this sort.}. 

There are many variations of explanatory non-inferentialism. So-called ``ontic'' theorists hold that explanations do not merely \textit{represent} these dependency relationships, but \textit{are} these dependency relationships. There is also considerable variation in which dependency relationships are the relevant ones. The most popular view holds them to be causal. Others take them to be/include statistical, mechanistic, supervenience, etc. relationships. For the purposes of this paper, we will take the chief form of explanatory non-inferentialism to be the thesis that explanations represent causal relationships. With slight modification, our discussion of this ``causal representationalist'' foil to explanatory inferentialism extends to the different variations of explanatory non-inferentialism just sketched above. 

In principle, non-inferentialists could hold that the explanatory relation is neither inferential nor ``dependency-representing.'' The most developed idea on this front comes from those who hold that the explanatory relationship is pragmatic in nature, e.g. it is the relationship of an answer to its corresponding why-question. We will mostly bracket this position here for three reasons. First, this pragmatic or ``erotetic'' position has been less prominent than the inferentialist or causal representationalist views discussed above. Second, there are ways of reconciling the pragmatic approach with the inferential approach, e.g. by arguing that any correct answer to a why-question must stand in the prescribed inferential relationships. Finally, the \textit{semantic} inferentialism that we will deploy here has traditionally been paired with a unique ``normative'' pragmatics, and, in future work, we hope to develop this pragmatic-semantic interface to our theory of explanation in greater detail.

\section{A New Approach to Explanatory Inferentialism}
\label{kernel}

The most dramatic way in which our approach to explanatory inferentialism differs from that of Hempel and Kitcher is in our conception of \textit{inference} itself. For the latter, \textit{inference} is fundamentally a relation between sentences in a language. For us, it is a  practical concept. First and foremost, \textit{inference} characterizes something that we \textit{do}; its reference to a sentential relation or function is derivative. The idea that drawing inferences is one of, if not \textit{the} essential ways in which we use language is central to the approach of semantic inferentialism pioneered by Wilfrid Sellars and developed in detail by figures such as Robert Brandom, and Jaroslav Peregrin. According to semantic inferentialism, the making of inferences is part of rule-governed practice---what Brandom calls, `a game of giving and asking for reasons.' In this game or practice, speakers are both players (making assertions and drawing inferences) and referees (keeping track of other's assertions and determining whether they are observing the rules of inference). But unlike other games, like \textit{Chess} or \textit{Simon Says}, the rules for the inference-game are not available to players as explicit instructions. Rather, the propriety of inferential moves is exhibited by the way other participants respond to those that make them. In this sense, the rules governing the game ---i.e. the rules of inference--- are \textit{implicit} in the practice of playing it. It is only because locutions can be introduced that permit speakers to explicitly endorse inferential moves that the notion of \textit{inference} as a relation among sentences can have any purchase.

Traditional explanatory inferentialism is of a piece with the classical project of analysis that animated Anglo-American philosophy for much of the twentieth century. In its most general sense, that project sought to show that the meanings expressed by one class of locutions could be expressed by another, more familiar, epistemologically secure, or semantically autonomous class of locutions. Hempel's account of explanation, for instance, aims to show how the meaning expressed by a class of explanatory locutions---i.e. indicative sentences containing the predicates ``\ldots explains\ldots,'' ``\ldots is a potential explanation of\ldots,'' ``\ldots is an adequate explanation of\ldots'' etc. as well as certain why-interrogative sentences---can be expressed by inferences among sentences belonging to a model language consisting of names for physical objects or spatio-temporal locations and so-called `qualitative' predicates. In contrast, our approach attempts to demonstrate that locutions such ``\ldots best explains \ldots'' enable speakers to overtly \textit{endorse} inferences that they could otherwise---i.e. using a non-explanatory language---only \textit{make}.\footnote{In this sense, our approach to explanatory vocabulary is a contribution to the program of analytic pragmatism described by Brandom. } This is what we mean when we say that explanatory vocabulary makes explicit inferential commitments.

For us, explanatory vocabulary makes explicit commitments to certain patterns of \textit{material} inference. Unlike the inferences codified by formal logic, material inferences are those whose correctness is not determined by the meaning and arrangement of logical terms---i.e. their `logical form'. The inference from ``Boston is north of Atlanta'' to ``Atlanta is south of Boston'' is an example of a good material inference. The formalist's insistence that this inference is enthymematic until the suppressed premise ``If x is north of y, then y is south of x'' is added simply trades the goodness of the material inference for the truth of the conditional. On our view, the inference is good as it stands and the supposedly `supressed' conditional is a way of making commitment to that inference explicit. 

The class of material inferences with which we are concerned consist of those that preserve \textit{entitlement} such that if one is entitled to (assert) the premises, then one is entitled to (assert) the conclusion. Such inferences are distinctly \textit{non-monotonic}. Ordinary scientific practice is replete with them---e.g. ``The liquid is acidic. So, it will turn blue litmus paper red.'' A good material inference of this sort might turn bad if additional premises or auxiliary hypotheses are added---e.g. ``Chlorine gas is present''\footnote{Chlorine gas bleaches damp litmus paper.}---and likewise, a bad inference might be made good by adding the right premises. 

It is our contention that the \textit{best explains} locution makes explicit a neatly circumscribed subset of these material inferences. We will call these target inferences \textit{explanatory}, though we ought to remember that they are rules for the use of expressions in a language which lacks explicit explanatory vocabulary. 

We begin with the insight that even though explanatory inferences are non-monotonic, they must nonetheless exhibit a range of \textit{subjunctive robustness}. An agent treats a material inference as having a certain range of subjunctive robustness when she can discriminate between those hypothetical circumstances in which the inference would remain good and those in which it would not. The paradigmatic way to express this range is by endorsing subjunctive conditionals of the form `If P were the case, then Q would be the case, even if R were the case,' or, `If P were the case, then Q would be the case, unless S were the case.'\footnote{These English conditionals are formed in the subjunctive mood. Such sentences are sometimes called \textit{counterfactual} conditionals. Other times, they are \textit{distinguished} from counterfactual conditionals, which are identified with sentences of the form `If P had been the case, then Q would have been the case.' In many languages, the antecedent of the latter sentence would be formulated in the past subjunctive, however, since English lacks a past subjunctive, the antecedent appears in the indicative mood with an extra layer of past tense morphology. We will use the term \textit{subjunctive} to pick out clauses of either type. In doing so, we reject the putative contrast between the two which claims that properly `counterfactual' conditionals presuppose that their antecedent is false, while simple `subjunctive' conditionals do not. As the following sentence shows, this is not always the case: ``If Susan had contracted influenza, then she would have presented exactly the symptoms that she does.''} Explanatory inferences persist in what we may think of as `islands of monotonicity'---they remain good under some hypothetical suppositions, but not others. 

On our view, explanatory inferences are distinguished from other types of `locally monotonic' material inferences by two characteristics. First, the (non-contextual) premises of explanatory inferences imply their conclusion with greater subjunctive robustness than any other premises that materially imply that conclusion (in the same context). In other words, the maximal set of subjunctive suppositions that can hold without defeating an explanatory inference is greater than that of any other material inference that has the same conclusion. Call this characteristic \textit{Superlative Robustness}. Second, the premises of explanatory inferences are `difference makers' in the sense that if the content of the latter were different, the content of the conclusion would be different as well. If, for instance, any sentence incompatible with the premises of a good explanatory inference were substituted for those premises, then the inference should yield something incompatible with the (original) conclusion.   Call this the \textit{Difference Maker} characteristic. \footnote{We view these characteristics as independently necessary but not jointly sufficient conditions for an explanatory inference. There is at least one more necessary condition that is required for an adequate analysis, namely, that explanatory inferences support the patterns of reasoning  known as Inference-to-the-Best-Explanation. Since this characteristic is orthogonal to the issues raised by the symmetry problem, we have omitted it from the present discussion.} 

\subsection{Representing Explanatory Inferences as Material Consequence Relations}

In order to perspicuously represent the defining characteristics of explanatory inference, we will construct a formal structure with a non-monotonic material consequence relation defined over a language of atomic sentences and proceed to circumscribe those instances of this relation that correspond to our explanatory inferences. We begin with a finite language $ \mathcal{L}_{0} $ devoid of logical vocabulary. Let $ p, p_1, p_2, $ etc. stand for atomic sentences, $ \Gamma, \Delta, \Theta $ for sets of atomic sentences, and $ W, W',W'' $ for sets of sets of atomic sentences. We reserve the uppercase Latin letters, $ A, B, C, D $ for formulas. In order to represent material incoherence, we extend $ \mathcal{L}_0 $ to include the constant $ \bigperp $, i.e. $ \mathcal{L} $ = $ \mathcal{L}_0 \,\bigcup \,\{\bigperp\}$. We further stipulate that $ \bigperp $ can neither appear to the left of the turnstile nor be embedded, i.e. $ {\nmc} \subseteq \mathcal{P}(\mathcal{L}_0) \times \mathcal{L} $. We can now define $ \nmc $ as the relation over $ \mathcal{L} $ such that $  \Gamma\nmc p $ iff  $\Gamma$ materially implies $ p $, and $\Gamma\nmc \bigperp $ iff $ \Gamma $ is materially incoherent. 

We now have in place an object language $ \mathcal{L} $ (an extension of  $ \mathcal{L}_0 $ that includes $ \bigperp $) and a meta-language that consists of $ \nmc $.  In the appendix, we formulate and explain the properties of the structure $ \langle \mathcal{L}, \nmc \rangle  $. For present purposes we simply note that $ \nmc $ is non-monotonic and non-transitive.  The most basic inferences with which we will be interested are of the form $ \Gamma, A \nmc B $.  We read this expression as stating that anyone who is committed and entitled to A in context $ \Gamma $ is entitled to B. Similarly, we read  $\Gamma, A, B \nmc \bigperp $ as stating that commitment and entitlement to A in context $ \Gamma $ precludes entitlement to B in context $ \Gamma $ and vice versa. Note that the comma is to be read as set-union with flanking individual formulae being read as in set brackets; e.g. ``$ A $'' on the left means ``$ \{ A\} $''.  We reserve $ \Gamma $ exclusively for a (non-empty) set of collateral commitments intended to represent the epistemic context of inference. As such, we assume that $ \Gamma $ contains information about, e.g., the experimental set-up, background knowledge relevant to the system in question, etc. \textcolor{red}{more}, but does not contain the agent's total knowledge of the world. 

Our first step toward modeling explanatory inferences in our meta-language is to represent the local monotonicity of material inferences. We do so by quantifying over our material consequence relation, $ \nmc $.   (For all of the following meta-theoretical claims, we omit reference to $\mathcal{L}$ and $\mathcal{P}(\mathcal{L})$ whenever doing so will not mislead the reader.)

	\begin{description}
		\item[\textbf{Quantified Material Consequence (QMC):}]
		\begin{equation}
              \qmc{A}{B}{W} \Longleftrightarrow_{df}
				      \begin{cases}\nonumber
				        1.\,\, W\subseteq\mathcal{P}(\mathcal{L}) & $ and $ \\[3pt] 
						2.\,\, \forall\Delta\in W(\Gamma, A,  \Delta \nmc B ) 
						\end{cases}
		\end{equation}
		
	\end{description}  
	
With this definition we now have a handy way of talking about sets of inferences. Note that since $ \emptyset \in W $, the second condition includes $ \Gamma, A \nmc B $. Call this the \textit{base inference} of the set of inferences. Sets of inferences are delimited by the set of sets of sentences of $ \mathcal{L} $ whose members may be added to the premises without impugning the base inference. The resulting set thus contains $ |W| $ many inferences,  and when $ W = \mathcal{P}(\mathcal{L}) $, the consequence relation in $\qmc{A}{B}{W} $ is globally monotonic.\footnote{The reason for circumscribing these sets of inferences by subsets of $ \mathcal{P}(\mathcal{L}) $ is that atomic sentences that would, by themselves materially defeat an inference if added to the premises, need not defeat that inference if they are add along with other atomic sentences.} Equivalently, we may think of $ \qmc{A}{B}{W}  $ as saying that the (base) inference from $ A $ to $ B $ \textit{would} remain materially good for an agent in context $ \Gamma $ even if she \textit{were} to undertake sets of collateral commitments from $ W $. For ease of exposition, we will often refer to $W$ as a set of \textit{subjunctive suppositions}.

Now that we have a way of representing local monotonicity, we need a way to depict the \textit{maximum range of subjunctive robustness} that a base inference exhibits. Giving expression to such maximum ranges in our meta-language enables us to compare base inferences in terms of the scope of their robustness, i.e. the `size' of their island of monotonicity.


	\begin{description}
		\item[\textbf{Subjunctively Robust Consequence (SRC):}]
		  \begin{equation}
		      \src{A}{B}{W}\Longleftrightarrow_{df}
		      \begin{cases}\nonumber
		        1.\,\, \qmc{A}{B}{W}& $ and $ \\[3pt] 
				2.\,\, \forall W'(\qmc{A}{B}{W'}\Longrightarrow W' \subseteq W)  & $ and $\\[3pt] 
				3.\,\, \nqmc[]{\Gamma}{B}{W} & $ and $\\[3pt]  
		        4.\,\, \forall\Delta(\qmc{A, \Delta}{\!\!\bigperp}{\mathcal{P}(\mathcal{L})}\Longrightarrow \Delta \not\in W)

				\end{cases}
		  \end{equation}

	\end{description}

The first condition of SRC tells us that the base inference $ \Gamma, A \nmc B $ would be licensed if any of the members of  $W$ were to be added to its premise-set, but the second condition says that $W$ contains \textit{all} those sets of sentences whose addition would not defeat the inference. The third condition prevents the conclusion from simply following from the context alone, while the fourth condition prohibits the elements of $W$ from being logically inconsistent with the premises, thereby ensuring that the conclusion follows non-trivially.  When the consequence relation $ \src[]{}{}{W} $ holds, we say that the base inference is subjunctively robust \textit{up to} $ W $. 

With SRC, we now have the means by which to compare the ranges of subjunctive robustness of material inferences that share the same conclusion. This means that we can identify those premises that materially imply a specified conclusion with the greatest range of subjunctive robustness, thereby capturing the \textit{Superlative Robustness} characteristic of explanatory inferences set out above. In order to represent \textit{Difference Making}, however, we need a way to specify the sets of sentences that are incompatible with a particular sentence, $A$, in a particular context, $\Gamma$,  under a set of subjunctive conditions, $W$. For this, we have SRIS.


\begin{description}
	\item[\textbf{Subjunctively Robust Incompatibility Set (SRIS):}]

	 $ D \in \sris{W}{\Gamma,\,A} \Longleftrightarrow_{df} \,\,\,\qmc{A, D}{\bigperp}{W} $

\end{description}

The set $  \sris{W}{\Gamma,\,A}  $contains all those formulas that are incompatible with $A$ in context $ \Gamma $ and that would continue to produce materially incoherence were any of the members of $W$ added to it. This definition provides the resources with which to express \textit{Difference Making}. We do so in the third condition of following definition.

\begin{description}
	\item[\textbf{Greatest Subjunctively Robust Consequence (GSRC):}]
		\begin{equation}
		   \gsrc{A}{\,B} \Longleftrightarrow_{df} 
		    \begin{cases}\nonumber 
			      1.\,\, \src{A}{B}{W} & $ and $ \\[3pt] 
				  2.\,\, \forall C(C \neq B)\,(\src{C}{B}{W'}\Longrightarrow W'\subseteq W ) & $ and $ \\[3pt] 
				  3.\,\, (\forall D_{\scriptscriptstyle A} \in \sris{W}{\Gamma,\,A}) (\exists!D_{\scriptscriptstyle B} \in \sris{W}{\Gamma,\,B})(\qmc{D_{\scriptscriptstyle A}}{D_{\scriptscriptstyle B}}{W})
			 \end{cases}
		\end{equation}

\end{description}

The GSRC relation is our meta-language expression for explanatory inference. The first condition says that the base inference is subjunctively robust up to $W$. The second condition tells us that $W$ is the largest set of subjunctive suppositions under which any sentence from $\mathcal{L}$ materially implies $B$, excluding $B$ itself. For this reason $W $ need not appear on the left-side of the definition. If we think of $ A $ as providing the best explanation of $ B $ and of $ C $ as any less-than-best explanation, then the second condition says that the best explanation is stable under more subjunctive suppositions than any of its suboptimal competitors. The third condition informs us that if any claim incompatible with $A$ were to replace $A$ in the premises, then it would materially imply a unique claim incompatible with $B$, under the subjunctive suppositions in $W$. Since the negation of a sentence can be construed (in an extended language) in terms of quantification over the set of sentences incompatible with that sentence, the third condition entails that if $A$ were not the case, $B$ would not be the case. 

\textcolor{red}{Need a bit more...}




\section{Classic EI and the Symmetry Problem} \label{sec:classic_symm}
The classical explanatory inferentialists face a variety of problems. How should laws be characterized? Can EI make sense of indeterministic explanations of low-probability events, such as the fact that a person's untreated syphilis explains his paresis, despite the fact that only 25\% of untreated syphilitics suffer from paresis? Can EI account for cases in which explanatorily irrelevant information does not affect deductive validity or inductive strength, for instance, when we deduce that a man is not pregnant because he has consumed birth control pills? What of problems unique to unificationism, such as its implausible epistemology, and the variety of non-explanatory unifications? While we hope to address these problems in future work, in this paper, we focus on what might well be the \textit{b\^{e}te noire} of classic EI: the symmetry problem.

The original symmetry problem, which targeted Hempel's view and is typically credited to Bromberger, is as follows: We have a good explanation of why a shadow is a particular length, which we can represent as follows:
\begin{equation} \label{eq:flagpole_expl}
	tan(\alpha) = h/l, \alpha = 60 \degree, h_{flagpole} = 25 \vdash l_{shadow} = 78 \footnote{Strictly speaking, the length of the shadow is an irrational number, 78.115140055... For simplicity's sake, we have bracketed this in what follows.}
\end{equation}
Here, $\alpha$ is the angle of incidence, $h$ denotes the height of an object, and $l$ denotes the length of that object's shadow.

Hempel's DN model correctly labels this as an explanation.The symmetry problem arises because Hempel's DN model is also satisfied by the following ``reverse explanation'':
\begin{equation} \label{eq:shadow_expl}
	tan(\alpha) = h/l, \alpha = 60  \degree, l_{shadow} = 78 \vdash h_{flagpole} = 25
\end{equation}

However, the length of the shadow does not explain the height of the flagpole. Hence, we have a deductive inference involving a law that is not an explanation, i.e. Hempel's covering law model fails to provide sufficient conditions on explanation.

\textcolor{red}{Should I discuss Kitcher's attempted solution? Potentially, this provides an occasion for us to head off the winner-take-all problem. For now, I'll keep it out.} Arguably, Kitcher's unificationism offers a solution to the flagpole counterexamples because the non-explanatory inferences do not fit patterns of inference that unify
a scientific domain. Nevertheless, Kitcher's view is not immune to all symmetry problems. In a critique of Kitcher's view, Barnes considers a closed system of ``Newtonian particles.''
Each particle has a position and velocity. Given a complete description of this system at a time, the state of the system at any later time can be determined through Newton's laws. Barnes argues that the deduction of each future state of the system from the present state satisfies Kitcher's criteria for explanation. Newton's laws, after all, are a paradigm of scientific unification. But Newton?s laws permit the deduction of past states from the present state as well. So, whatever features Newton's laws exhibit that let them fit Kitcher's criteria, both the forward and backward calculations must count as explanatory. But clearly, calculation of past states does not count as an explanation.

Let's render Barnes' example more concrete. Consider a simple system consisting of two billiard balls, $A$ and $B$, which are of identical mass. These are Barnes' Newtonian particles. The initial velocity of $A$ is $V_{1A} \neq 0$. $A$ collides with $B$, the latter of which is at rest (i.e., $V_{1B} = 0$). After impact, $A$ moves at velocity $V_{2A}$, and $B$ moves at velocity $V_{2B}$. Newtonian mechanics entails:
\begin{equation} \label{eq:newtonian}
	(\vec V_{1A})^{2} = (\vec V_{2A})^{2} + (\vec V_{2B})^{2}
\end{equation}
Assume that this equation will be part of Kitcher's maximally unifying ``explanatory store'' $E(K)$. We can then offer a vivid example of Barnes' objection. First, Kitcher's unificationism correctly implies that the velocity of $A$ before its collision with $B$ explains the velocity of $A$ after this collision, i.e.
\begin{equation} \label{eq:good_billiard_expl}
	E(K), \vec V_{1A} = <x_1, y_1>, \vec V_{2B} = <x_2, y_2> \,\,\vdash \, \vec V_{2A} = <x_3, y_3>
\end{equation}
However, Kitcher's view also licenses the following:
\begin{equation} \label{eq:bad_billiard_expl}
	E(K), \vec V_{2A} = <x_3, y_3>, \vec V_{2B} = <x_2, y_2> \,\,\vdash\, \vec V_{1A} = <x_1, y_1>
\end{equation}
Just as Hempel's view incorrectly entails that the shadow's length explains the flagpole's height, Kitcher's view incorrectly entails that the velocity of $A$ after the collision with $B$ explains its velocity before this collision. This nicely illustrates the force of Barnes' objection.






\section{The Symmetry Problem Solved}
\label{symm_solution}

In Section \ref{sec:classic_symm}, we saw that earlier proponents of EI failed to account for the asymmetry of explanation. Specifically, while Hempel's view correctly identified the flagpole's height as explaining the shadow's length, i.e. \eqref{eq:flagpole_expl}, it also incorrectly identified the shadow's length as explaining the flagpole's height \eqref{eq:shadow_expl}. Similarly, Kitcher's unificationism correctly identified billiard ball $A$'s velocity prior to its collision with $B$ as explaining $A$'s subsequent velocity \eqref{eq:good_billiard_expl}, but incorrectly identified the converse \eqref{eq:bad_billiard_expl} as an explanation.

By contrast, we shall argue that our new version of EI ably solves these asymmetry problems. Our solution proceeds in two steps. First, in Section \ref{subsec:illusion}, we argue that while the inferences above--\eqref{eq:flagpole_expl}, \eqref{eq:shadow_expl}, \eqref{eq:good_billiard_expl}, and \eqref{eq:bad_billiard_expl}--are all \textit{presented} as deductive (and hence monotonic) inferences, in \textit{practice}, they are in fact non-monotonic inferences. Once this point is appreciated, it is seen that the problematic ``reverse explanations,'' i.e. \eqref{eq:shadow_expl} and \eqref{eq:bad_billiard_expl}, are merely SRCs, but are not GSRCs, as we argue in Section \ref{subsec:not_great}. Hence, unlike our predecessors, our view can capture the asymmetry of explanation.

\subsection{The Illusion of Montonicity}
\label{subsec:illusion}

At first  blush, it would appear that our position fares no better than its predecessors with respect to the symmetry problem. Both \eqref{eq:shadow_expl} and \eqref{eq:bad_billiard_expl} are deductive inferences. Hence, they readily satisfy GSRC1 and GSRC2, for they will be good inferences for any possible value of $W$. Furthermore, different values of $\alpha$ and $l_{shadow}$ will furnish different values of $h_{flagpole}$, and different values of $\vec{V}_{2A}$ and $\vec{V}_{2A}$ will furnish different values of $\vec{V}_{1A}$, so these two `reverse explanations" will also satisfy GSRC3. So, it would appear that the symmetry problem bedevils our position no less than it bedeviled Hempel and Kitcher. \newline
\indent However, these initial appearances are misleading. All of the inferences in Section \ref{sec:classic_symm} are deductive, and hence monotonic. While scientists frequently \textit{present} these inferences in deductive form, closer investigation of scientific practice suggests that the inferences, at their core, are non-montonic. For instance, consider the use of the trigonometric law to calculate the length of a shadow. The law breaks down when the object is transparent \footnote{Objects that are perfectly transparent cast no shadows. However, many objects that are nearly transparent (e.g. glasses of water), do cast shadows. Throughout this discussion, ``transparent'' is shorthand for ``perfectly transparent.''}. Scientists know this, and simply do not use the law in these cases. Let $t$ be the sentence, `The flagpole is transparent.'' Whenever $t$ obtains, the premises in \eqref{eq:flagpole_expl} and \eqref{eq:shadow_expl} will continue to entail their respective conclusions, but the conclusions will be false. Scientists know this, and hence do not use these inferences when the objects in question are transparent. Similarly, let $o$ be the sentence, ``There are oil slicks on $\vec{V}_{2A}$'s path.'' Then parallel points will apply to \eqref{eq:good_billiard_expl} and \eqref{eq:bad_billiard_expl}.\newline
\indent In other words, the trigonometric law in \eqref{eq:flagpole_expl} and \eqref{eq:shadow_expl}, and the Newtonian law, \eqref{eq:newtonian}, that figures in \eqref{eq:good_billiard_expl} and \eqref{eq:bad_billiard_expl} are naturally regarded as \textit{ceteris paribus} laws. We shall represent this added twist as follows:
\begin{equation}
cp(tan(\alpha) = h/l)
\end{equation}
For the sake of compactness, we shall represent this as as $cp(tan)$. Similarly, $cp(\vec{V})$ shall indicate that \eqref{eq:newtonian} is to be represented as a \textit{ceteris paribus} law. Intuitively, inferences involving \textit{ceteris paribus} are non-monotonic: we have reason to resist the conclusion when \textit{ceteris} is not \textit{paribus}, even when the premises obtain. Consequently, the inferences in Section \ref{sec:classic_symm} should be recast as follows:
\begin{equation} \label{eq:src_flagpole_expl}
	\src{cp(tan), \alpha = 60 \degree, h_{flagpole} = 25}{l_{shadow} = 78}{W}
\end{equation}
\begin{equation} \label{eq:src_shadow_expl}
	\src{cp(tan), \alpha = 60 \degree, l_{shadow} = 78}{h_{flagpole} = 25}{W}
\end{equation}
\begin{equation} \label{eq:src_good_billiard_expl}
	\src{cp(\vec{V}), \vec V_{1A} = <x_1, y_1>, \vec V_{2B} = <x_2, y_2>\,}{\vec V_{2A} = <x_3, y_3>}{W}
\end{equation}
\begin{equation} \label{eq:src_bad_billiard_expl}
	\src{cp(\vec{V}),\vec V_{2A} = <x_3, y_3>, \vec V_{2B} = <x_2, y_2>\,}{\vec V_{1A} = <x_1, y_1>}{W}
\end{equation}
Moreover, there are conditions, such as $t$ and $o$, that render these inferences into bad ones. For instance, if $t$ obtains, the $l_{shadow} = 0$, which falsely implies that $h_{flagpole}=0$, but of course, the flagpole's height is independent of its transparency. Parallel points apply to $o$ and \eqref{eq:src_bad_billiard_expl}. In our locution, this means that $t$ and $o$ cannot be members of $W$ in these inferences. Thus, these are full-blooded non-monotonic inferences. So, to solve the symmetry problem, we must show that \eqref{eq:src_flagpole_expl} and \eqref{eq:src_good_billiard_expl} are GSRCs, while the reverse explanations, now represented by \eqref{eq:src_shadow_expl} and \eqref{eq:src_bad_billiard_expl}, are not.\newline


\subsection{Reverse Explanations Are Not So Great (SRCs)}
\label{subsec:not_great}

Let's begin by showing that our reverse explanations, \eqref{eq:src_shadow_expl} and \eqref{eq:src_bad_billiard_expl}, are not GSRCs. Both are SRCs, and thereby satisfy GSRC1: there are conditions $W$ in which we can use the relevant \textit{ceteris paribus} laws and initial conditions to calculate flagpole heights and past velocities. Moreover, it is still the case that different values of $\alpha$ and $l_{shadow}$ will furnish different values of $h_{flagpole}$, and different values of $\vec{V}_{2A}$ and $\vec{V}_{2B}$ will furnish different values of $\vec{V}_{1A}$, so these two reverse explanations still satisfy GSRC3. However, we shall now argue that both inferences fail to be GSRCs because they do not satisfy GSRC2. \newline
\indent To see this, first consider the flagpole example. However, unlike before, consider an alternative explanation $C$ of the flagpole's height, namely that it was \textit{built} to be 25 units high:
\begin{equation} \label{eq:built_expl}
	\src{C}{h_{flagpole} = 25}{W'}
\end{equation}
Finally, observe that $t$, which states that the flagpole is transparent, is a member $W'$: even if a transparent flagpole were designed to be 25 units high, it would still be 25 units high. By contrast, and as already noted, $t$ is not a member of $W$ in \eqref{eq:src_shadow_expl}. Hence, the latter fails to satisfy GSRC2, and is thereby not a GSRC.\newline
\indent For similar reasons, inferring the velocity of $A$ prior to its collision with $B$ from $A$'s velocity after this collision, i.e. \eqref{eq:src_bad_billiard_expl}, is also not a GSRC because of its failure to satisfy GSRC2. Consider an alternative explanation of why $\vec{V}_{1A}=<x_1, y_1>$, namely that it was struck with a cue prior to its embarking on the trajectory that would ultimately result in its collision with $B$. As before, call this alternative explanation $C$. This results in the following:
\begin{equation}\label{eq:cue_expl}
	\src{C}{\vec V_{1A} = <x_1, y_1>}{W'}
\end{equation}
Observe that $o$, which states that there are oil slicks on on $\vec{V}_{2A}$'s path, is a member of $W'$, for even if Ball $A$ hits the skids after its collision with Ball $B$, its velocity prior to that collision will remain unchanged if it is struck by the cue. However, $o$ is not a member of the $W$ to which \eqref{eq:src_bad_billiard_expl} refers. Hence, the latter does not satisfy GSRC2, and is thereby not a GSRC. \newline
\indent Hence, we have seen that our view readily dispenses with the problematic reverse explanations that beget the symmetry problem. However, this solution will be for naught if it also dispenses with the correct explanations of the shadow's length \eqref{eq:src_flagpole_expl}, and of Ball $A$'s velocity after its collision with $B$ \eqref{eq:src_good_billiard_expl}. As should be clear, both of these explanations satisfy GSRC1 and GSRC3. Moreover, $t$ will not be a member of $W$ in \eqref{eq:src_flagpole_expl}, nor $o$ will be a member of $W$ in \eqref{eq:src_good_billiard_expl}. In this regard, they are analogous to the reverse explanations. However, as a first pass, there are no obvious analogues to \eqref{eq:built_expl} and \eqref{eq:cue_expl} with these inferences. In other words, there are no alternatives $C$ from which the shadow's length can be inferred regardless of the transparency or opacity of the flagpole; nor is there an alternative from which Ball $A$'s post-collision velocity can be inferred regardless of the presence or absence of oil slicks. Hence, there are prima facie good reasons for thinking that \eqref{eq:src_flagpole_expl} and \eqref{eq:src_good_billiard_expl} are GSRCs. Thus, while both these and their reverse counterparts are defeasible or non-monotonic inferences, only the reverse explanations have alternative explanations that can `withstand' these defeaters.\newline
\indent \textcolor{red}{If we were to put in more stuff about Kitcher in Section \ref{sec:classic_symm}, then we could talk about fundamental and derivative explanations, as I sketched in my previous document.}


\section{Compliance with Ethical Standards}
The author declares no potential conflicts of interests with respect to the authorship and/or publication of this article. The author received no financial support for the research and/or authorship of this article.


\bibliographystyle{spbasic}
\bibliography{Explanation_bib} 

% % % % % % % % % % % % % % % % % % % % % % % For submission use bib below and paste bibitems % % % % % % % % % % % % % % % % %
%\begin{thebibliography}{19}
%\providecommand{\natexlab}[1]{#1}
%\providecommand{\url}[1]{{#1}}
%\providecommand{\urlprefix}{URL }
%\expandafter\ifx\csname urlstyle\endcsname\relax
%  \providecommand{\doi}[1]{DOI~\discretionary{}{}{}#1}\else
%  \providecommand{\doi}{DOI~\discretionary{}{}{}\begingroup
%  \urlstyle{rm}\Url}\fi
%\providecommand{\eprint}[2][]{\url{#2}}
%
%%\bibitem[{Goldberg(2011)}]{Goldberg2011}
%%Goldberg S (2011) Putting the norm of assertion to work: The case of testimony.
%%  In: Brown J, Cappelen H (eds) Assertion: New Philosophical Essays, Oxford
%%  University Press
%
%
%
%\end{thebibliography}

\end{document}